{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "**Brace yourselves**! This is going to be a fun and challenging assignment.\n",
    "\n",
    "\n",
    "* Use SFrames to do some feature engineering.\n",
    "* Modify the decision trees to incorporate weights.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create **(1.8.3 or newer)**. Upgrade by\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to raghuveernamala@gmail.com and will expire on January 08, 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1519695036.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same [LendingClub](https://www.lendingclub.com/) dataset as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = graphlab.SFrame('../week3/lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target and the feature columns\n",
    "\n",
    "We will now repeat some of the feature processing steps that we saw in the previous assignment:\n",
    "\n",
    "First, we re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan.\n",
    "\n",
    "Next, we select four categorical features: \n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Using default 16 lambda workers.</pre>"
      ],
      "text/plain": [
       "Using default 16 lambda workers."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>To maximize the degree of parallelism, add the following code to the beginning of the program:</pre>"
      ],
      "text/plain": [
       "To maximize the degree of parallelism, add the following code to the beginning of the program:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>\"graphlab.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 24)\"</pre>"
      ],
      "text/plain": [
       "\"graphlab.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 24)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Note that increasing the degree of parallelism also increases the memory footprint.</pre>"
      ],
      "text/plain": [
       "Note that increasing the degree of parallelism also increases the memory footprint."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.remove_column('bad_loans')\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.502236174422\n",
      "Percentage of risky loans                : 0.497763825578\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed=1)\n",
    "loans_data = risky_loans_raw.append(safe_loans)\n",
    "\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are many approaches for dealing with imbalanced data, including some where we modify the learning algorithm. These approaches are beyond the scope of this course, but some of them are reviewed in this [paper](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5128907&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F69%2F5173046%2F05128907.pdf%3Farnumber%3D5128907 ). For this assignment, we use the simplest possible approach, where we subsample the overly represented class to get a more balanced dataset. In general, and especially when the data is highly imbalanced, we recommend using more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block (see the first assignments for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our decision tree code from Module 5 to support weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "#     print('total_weight_positive : ', total_weight_positive)\n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "#     print('total_weight_negative : ', total_weight_negative)\n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = sum(data_weights[labels_in_node == 1])\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    if total_weight_positive <= total_weight_negative:\n",
    "        weight = weighted_mistakes_all_positive\n",
    "        label = -1\n",
    "    else:\n",
    "        weight = weighted_mistakes_all_negative\n",
    "        label = +1\n",
    "#     print(weight, label)    \n",
    "    return (weight, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = graphlab.SArray([-1, -1, 1, 1, 1])\n",
    "example_data_weights = graphlab.SArray([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "**Quiz Question:** If we set the weights $\\mathbf{\\alpha} = 1$ for all data points, how is the weight of mistakes $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ related to the `classification error`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue modifying our decision tree code from the earlier assignment to incorporate weighting of individual data points. The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function is similar to the one from the earlier assignment with two minor modifications:\n",
    "  1. The function **best_splitting_feature** should now accept an extra parameter `data_weights` to take account of weights of data points.\n",
    "  2. Instead of computing the number of mistakes in the left and right side of the split, we compute the weight of mistakes for both sides, add up the two weights, and divide it by the total weight of the data.\n",
    "  \n",
    "Complete the following function. Comments starting with `DIFFERENT HERE` mark the sections where the weighted version differs from the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "                    \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = ( left_weighted_mistakes + right_weighted_mistakes ) / float(sum(data_weights))\n",
    "#         print('best_error: ', best_error)\n",
    "#         print('best_feature :', best_feature)\n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term. 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. If you get an exception in the line of \"the logical filter has different size than the array\", try upgradting your GraphLab Create installation to 1.8.3 or newer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very Optional**. Relationship between weighted error and weight of mistakes\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Recall from the previous assignments that each node in the decision tree is represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] =  best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.D'},\n",
       " 'splitting_feature': 'term. 36 months'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "Split on feature grade.F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "Split on feature grade.D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = graphlab.SArray([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training.\n",
    "\n",
    "**Quiz Question**: Will you get the same model as `small_data_decision_tree_subset_20` if you trained a decision tree with only the 20 data points with non-zero weights from the set of points in `subset_20`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = graphlab.SArray([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weighted_error = alpha[is_wrong].sum() / alpha.sum()\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight = 0.5 * log ((1- weighted_error)/ weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha *= adjustment\n",
    "        alpha = alpha / alpha.sum()\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]            [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]                    [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15802933659263743, 0.17682363293636694]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = graphlab.SArray([0.]*len(data))\n",
    "    print('scores shape: ', scores.shape)\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        # YOUR CODE HERE\n",
    "#         print('intermediate weighted prediction : ', stump_weights[i] * predictions)\n",
    "#         print('scores before: ', scores)\n",
    "        scores += stump_weights[i] * predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('scores shape: ', (9284,))\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, 0.17682363293636694, -0.17682363293636694, 0.17682363293636694, 0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, -0.17682363293636694, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, 0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, 0.15802933659263743, -0.15802933659263743, -0.15802933659263743, -0.15802933659263743, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, -0.0931188897118425, 0.0931188897118425, 0.0931188897118425, 0.0931188897118425, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.33485296952900434, 0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, 0.33485296952900434, -0.018794296343729505, -0.018794296343729505, 0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, 0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, 0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.018794296343729505, 0.33485296952900434, -0.018794296343729505, -0.33485296952900434, 0.33485296952900434, -0.33485296952900434, -0.018794296343729505, 0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, 0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, 0.33485296952900434, -0.018794296343729505, 0.018794296343729505, 0.33485296952900434, -0.018794296343729505, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.018794296343729505, -0.33485296952900434, -0.33485296952900434, -0.33485296952900434, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, -0.07288885525865149, -0.07288885525865149, 0.07288885525865149, -0.07288885525865149, 0.07288885525865149, 0.07288885525865149, 0.07288885525865149, -0.07288885525865149, 0.07288885525865149, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.42797185924084685, 0.42797185924084685, -0.24173407981716183, -0.42797185924084685, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, -0.111913186055572, -0.111913186055572, -0.24173407981716183, 0.42797185924084685, 0.07432459336811299, -0.111913186055572, 0.42797185924084685, 0.07432459336811299, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, -0.111913186055572, 0.07432459336811299, 0.42797185924084685, -0.24173407981716183, -0.42797185924084685, -0.24173407981716183, 0.07432459336811299, 0.07432459336811299, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, 0.07432459336811299, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, 0.42797185924084685, -0.24173407981716183, 0.07432459336811299, 0.07432459336811299, 0.07432459336811299, 0.42797185924084685, 0.07432459336811299, -0.24173407981716183, 0.42797185924084685, -0.24173407981716183, 0.07432459336811299, 0.42797185924084685, -0.24173407981716183, -0.42797185924084685, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, 0.07432459336811299, 0.07432459336811299, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, 0.07432459336811299, 0.07432459336811299, -0.111913186055572, -0.24173407981716183, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, 0.42797185924084685, -0.42797185924084685, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, -0.24173407981716183, -0.24173407981716183, -0.111913186055572, -0.24173407981716183, 0.07432459336811299, 0.42797185924084685, 0.07432459336811299, 0.111913186055572, 0.42797185924084685, 0.07432459336811299, -0.111913186055572, -0.24173407981716183, -0.24173407981716183, 0.07432459336811299, -0.42797185924084685, -0.24173407981716183, -0.111913186055572, -0.24173407981716183, -0.111913186055572, -0.24173407981716183, -0.24173407981716183, -0.24173407981716183, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, 0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, 0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, 0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, 0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, -0.06706306914162356, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5008607144994983, 0.5008607144994983, -0.3146229350758133, -0.5008607144994983, 0.0014357381094615013, -0.3146229350758133, -0.3146229350758133, 0.1472134486267645, -0.1848020413142235, -0.1848020413142235, -0.3146229350758133, 0.3550830039821954, 0.0014357381094615013, -0.1848020413142235, 0.3550830039821954, 0.1472134486267645, 0.0014357381094615013, -0.3146229350758133, -0.3146229350758133, -0.1848020413142235, 0.1472134486267645, 0.5008607144994983, -0.16884522455851034, -0.5008607144994983, -0.3146229350758133, 0.0014357381094615013, 0.0014357381094615013, -0.16884522455851034, 0.1472134486267645, -0.16884522455851034, -0.3146229350758133, -0.3146229350758133, -0.3146229350758133, 0.0014357381094615013, 0.0014357381094615013, -0.16884522455851034, 0.1472134486267645, -0.3146229350758133, 0.3550830039821954, -0.3146229350758133, 0.0014357381094615013, 0.0014357381094615013, 0.0014357381094615013, 0.3550830039821954, 0.0014357381094615013, -0.3146229350758133, 0.3550830039821954, -0.16884522455851034, 0.0014357381094615013, 0.3550830039821954, -0.3146229350758133, -0.5008607144994983, -0.16884522455851034, 0.1472134486267645, -0.16884522455851034, 0.0014357381094615013, 0.0014357381094615013, 0.0014357381094615013, -0.16884522455851034, -0.3146229350758133, 0.0014357381094615013, -0.3146229350758133, 0.1472134486267645, -0.16884522455851034, 0.0014357381094615013, 0.1472134486267645, -0.1848020413142235, -0.3146229350758133, -0.16884522455851034, -0.3146229350758133, 0.0014357381094615013, 0.3550830039821954, -0.5008607144994983, 0.0014357381094615013, -0.16884522455851034, -0.3146229350758133, -0.3146229350758133, 0.1472134486267645, -0.3146229350758133, -0.16884522455851034, -0.1848020413142235, -0.3146229350758133, 0.1472134486267645, 0.3550830039821954, 0.1472134486267645, 0.1848020413142235, 0.3550830039821954, 0.0014357381094615013, -0.1848020413142235, -0.16884522455851034, -0.3146229350758133, 0.0014357381094615013, -0.5008607144994983, -0.16884522455851034, -0.1848020413142235, -0.16884522455851034, -0.03902433079692051, -0.16884522455851034, -0.3146229350758133, -0.16884522455851034, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, -0.06456916961624358, -0.06456916961624358, 0.06456916961624358, -0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, -0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, 0.06456916961624358, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5679237836411218, 0.43379764535787474, -0.38168600421743687, -0.5679237836411218, 0.06849880725108506, -0.38168600421743687, -0.24755986593418972, 0.08015037948514094, -0.2518651104558471, -0.2518651104558471, -0.38168600421743687, 0.2880199348405718, 0.06849880725108506, -0.2518651104558471, 0.2880199348405718, 0.21427651776838808, 0.06849880725108506, -0.24755986593418972, -0.38168600421743687, -0.2518651104558471, 0.21427651776838808, 0.43379764535787474, -0.10178215541688677, -0.5679237836411218, -0.38168600421743687, 0.06849880725108506, 0.06849880725108506, -0.2359082937001339, 0.08015037948514094, -0.2359082937001339, -0.38168600421743687, -0.38168600421743687, -0.38168600421743687, -0.06562733103216206, 0.06849880725108506, -0.2359082937001339, 0.08015037948514094, -0.38168600421743687, 0.2880199348405718, -0.38168600421743687, 0.06849880725108506, 0.06849880725108506, 0.06849880725108506, 0.2880199348405718, -0.06562733103216206, -0.38168600421743687, 0.2880199348405718, -0.10178215541688677, 0.06849880725108506, 0.2880199348405718, -0.24755986593418972, -0.5679237836411218, -0.2359082937001339, 0.08015037948514094, -0.2359082937001339, -0.06562733103216206, 0.06849880725108506, 0.06849880725108506, -0.10178215541688677, -0.38168600421743687, 0.06849880725108506, -0.38168600421743687, 0.21427651776838808, -0.2359082937001339, -0.06562733103216206, 0.08015037948514094, -0.2518651104558471, -0.38168600421743687, -0.2359082937001339, -0.38168600421743687, -0.06562733103216206, 0.2880199348405718, -0.5679237836411218, -0.06562733103216206, -0.2359082937001339, -0.38168600421743687, -0.38168600421743687, 0.21427651776838808, -0.38168600421743687, -0.2359082937001339, -0.2518651104558471, -0.38168600421743687, 0.08015037948514094, 0.2880199348405718, 0.08015037948514094, 0.11773897217259995, 0.2880199348405718, -0.06562733103216206, -0.2518651104558471, -0.10178215541688677, -0.38168600421743687, 0.06849880725108506, -0.5679237836411218, -0.2359082937001339, -0.2518651104558471, -0.10178215541688677, -0.10608739993854407, -0.2359082937001339, -0.38168600421743687, -0.2359082937001339, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, 0.05456055779184307, -0.05456055779184307, 0.05456055779184307, 0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, -0.05456055779184307, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5033546140248782, 0.49836681497411833, -0.3171168346011933, -0.5033546140248782, 0.13306797686732863, -0.3171168346011933, -0.18299069631794612, 0.14471954910138451, -0.1872959408396035, -0.1872959408396035, -0.44625517383368046, 0.3525891044568154, 0.13306797686732863, -0.1872959408396035, 0.3525891044568154, 0.27884568738463167, 0.13306797686732863, -0.18299069631794612, -0.44625517383368046, -0.1872959408396035, 0.27884568738463167, 0.49836681497411833, -0.037212985800643195, -0.5033546140248782, -0.44625517383368046, 0.13306797686732863, 0.13306797686732863, -0.3004774633163775, 0.14471954910138451, -0.1713391240838903, -0.3171168346011933, -0.3171168346011933, -0.3171168346011933, -0.0010581614159184821, 0.13306797686732863, -0.1713391240838903, 0.14471954910138451, -0.3171168346011933, 0.3525891044568154, -0.44625517383368046, 0.13306797686732863, 0.13306797686732863, 0.13306797686732863, 0.3525891044568154, -0.0010581614159184821, -0.3171168346011933, 0.3525891044568154, -0.037212985800643195, 0.13306797686732863, 0.3525891044568154, -0.18299069631794612, -0.5033546140248782, -0.3004774633163775, 0.14471954910138451, -0.1713391240838903, -0.0010581614159184821, 0.13306797686732863, 0.13306797686732863, -0.037212985800643195, -0.44625517383368046, 0.13306797686732863, -0.44625517383368046, 0.27884568738463167, -0.3004774633163775, -0.0010581614159184821, 0.14471954910138451, -0.1872959408396035, -0.44625517383368046, -0.3004774633163775, -0.3171168346011933, -0.0010581614159184821, 0.3525891044568154, -0.5033546140248782, -0.0010581614159184821, -0.3004774633163775, -0.44625517383368046, -0.44625517383368046, 0.27884568738463167, -0.44625517383368046, -0.3004774633163775, -0.1872959408396035, -0.3171168346011933, 0.14471954910138451, 0.3525891044568154, 0.14471954910138451, 0.18230814178884353, 0.3525891044568154, -0.0010581614159184821, -0.1872959408396035, -0.037212985800643195, -0.3171168346011933, 0.13306797686732863, -0.5033546140248782, -0.3004774633163775, -0.1872959408396035, -0.037212985800643195, -0.041518230322300495, -0.1713391240838903, -0.3171168346011933, -0.1713391240838903, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.043510936733700704, 0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, 0.043510936733700704, -0.043510936733700704, -0.043510936733700704, -0.043510936733700704, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5579151718167213, 0.5529273727659614, -0.3716773923930363, -0.5579151718167213, 0.07850741907548556, -0.3716773923930363, -0.2375512541097892, 0.09015899130954144, -0.24185649863144656, -0.24185649863144656, -0.5008157316255235, 0.40714966224865845, 0.07850741907548556, -0.24185649863144656, 0.40714966224865845, 0.2242851295927886, 0.07850741907548556, -0.2375512541097892, -0.5008157316255235, -0.24185649863144656, 0.2242851295927886, 0.5529273727659614, -0.09177354359248627, -0.5579151718167213, -0.5008157316255235, 0.07850741907548556, 0.07850741907548556, -0.3550380211082206, 0.09015899130954144, -0.22589968187573337, -0.3716773923930363, -0.3716773923930363, -0.3716773923930363, -0.055618719207761555, 0.07850741907548556, -0.22589968187573337, 0.09015899130954144, -0.3716773923930363, 0.40714966224865845, -0.5008157316255235, 0.07850741907548556, 0.07850741907548556, 0.07850741907548556, 0.40714966224865845, -0.055618719207761555, -0.3716773923930363, 0.40714966224865845, -0.09177354359248627, 0.07850741907548556, 0.40714966224865845, -0.2375512541097892, -0.5579151718167213, -0.3550380211082206, 0.09015899130954144, -0.22589968187573337, -0.055618719207761555, 0.07850741907548556, 0.07850741907548556, -0.09177354359248627, -0.5008157316255235, 0.07850741907548556, -0.5008157316255235, 0.2242851295927886, -0.3550380211082206, -0.055618719207761555, 0.09015899130954144, -0.24185649863144656, -0.5008157316255235, -0.3550380211082206, -0.3716773923930363, -0.055618719207761555, 0.40714966224865845, -0.5579151718167213, -0.055618719207761555, -0.3550380211082206, -0.5008157316255235, -0.5008157316255235, 0.2242851295927886, -0.5008157316255235, -0.3550380211082206, -0.24185649863144656, -0.3716773923930363, 0.09015899130954144, 0.40714966224865845, 0.09015899130954144, 0.2368686995806866, 0.40714966224865845, -0.055618719207761555, -0.24185649863144656, -0.09177354359248627, -0.3716773923930363, 0.07850741907548556, -0.5579151718167213, -0.3550380211082206, -0.24185649863144656, -0.09177354359248627, -0.09607878811414357, -0.22589968187573337, -0.3716773923930363, -0.22589968187573337, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, 0.02898871150035158, -0.02898871150035158, 0.02898871150035158, 0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, -0.02898871150035158, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5144042350830207, 0.596438309499662, -0.41518832912673703, -0.5144042350830207, 0.12201835580918627, -0.41518832912673703, -0.19404031737608848, 0.13366992804324215, -0.19834556189774585, -0.19834556189774585, -0.4573047948918228, 0.45066059898235916, 0.12201835580918627, -0.19834556189774585, 0.45066059898235916, 0.2677960663264893, 0.12201835580918627, -0.19404031737608848, -0.4573047948918228, -0.19834556189774585, 0.2677960663264893, 0.596438309499662, -0.04826260685878556, -0.5144042350830207, -0.4573047948918228, 0.12201835580918627, 0.12201835580918627, -0.31152708437451987, 0.13366992804324215, -0.26941061860943405, -0.41518832912673703, -0.3281664556593356, -0.3281664556593356, -0.01210778247406085, 0.12201835580918627, -0.26941061860943405, 0.13366992804324215, -0.3281664556593356, 0.45066059898235916, -0.4573047948918228, 0.12201835580918627, 0.12201835580918627, 0.12201835580918627, 0.45066059898235916, -0.01210778247406085, -0.3281664556593356, 0.45066059898235916, -0.04826260685878556, 0.12201835580918627, 0.45066059898235916, -0.19404031737608848, -0.5144042350830207, -0.31152708437451987, 0.13366992804324215, -0.18238874514203265, -0.01210778247406085, 0.12201835580918627, 0.12201835580918627, -0.04826260685878556, -0.4573047948918228, 0.12201835580918627, -0.4573047948918228, 0.2677960663264893, -0.31152708437451987, -0.01210778247406085, 0.13366992804324215, -0.19834556189774585, -0.4573047948918228, -0.31152708437451987, -0.41518832912673703, -0.01210778247406085, 0.45066059898235916, -0.5144042350830207, -0.01210778247406085, -0.31152708437451987, -0.4573047948918228, -0.4573047948918228, 0.2677960663264893, -0.4573047948918228, -0.31152708437451987, -0.19834556189774585, -0.3281664556593356, 0.13366992804324215, 0.45066059898235916, 0.13366992804324215, 0.2803796363143873, 0.45066059898235916, -0.01210778247406085, -0.19834556189774585, -0.04826260685878556, -0.41518832912673703, 0.12201835580918627, -0.5144042350830207, -0.31152708437451987, -0.19834556189774585, -0.04826260685878556, -0.052567851380442863, -0.26941061860943405, -0.41518832912673703, -0.26941061860943405, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, -0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, -0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, 0.025962509691371802, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5433929465833722, 0.6254270210000136, -0.4441770406270886, -0.5433929465833722, 0.09302964430883469, -0.4441770406270886, -0.22302902887644005, 0.10468121654289057, -0.22733427339809742, -0.22733427339809742, -0.4862935063921744, 0.47964931048271076, 0.09302964430883469, -0.22733427339809742, 0.47964931048271076, 0.23880735482613769, 0.09302964430883469, -0.22302902887644005, -0.4862935063921744, -0.22733427339809742, 0.23880735482613769, 0.6254270210000136, -0.07725131835913715, -0.5433929465833722, -0.4862935063921744, 0.09302964430883469, 0.09302964430883469, -0.34051579587487146, 0.10468121654289057, -0.29839933010978564, -0.4441770406270886, -0.3571551671596872, -0.3571551671596872, -0.04109649397441243, 0.09302964430883469, -0.29839933010978564, 0.10468121654289057, -0.3571551671596872, 0.47964931048271076, -0.4862935063921744, 0.09302964430883469, 0.09302964430883469, 0.09302964430883469, 0.47964931048271076, -0.04109649397441243, -0.3571551671596872, 0.47964931048271076, -0.07725131835913715, 0.09302964430883469, 0.47964931048271076, -0.22302902887644005, -0.5433929465833722, -0.34051579587487146, 0.10468121654289057, -0.21137745664238422, -0.04109649397441243, 0.09302964430883469, 0.09302964430883469, -0.07725131835913715, -0.4862935063921744, 0.09302964430883469, -0.4862935063921744, 0.23880735482613769, -0.34051579587487146, -0.04109649397441243, 0.10468121654289057, -0.22733427339809742, -0.4862935063921744, -0.34051579587487146, -0.4441770406270886, -0.04109649397441243, 0.47964931048271076, -0.5433929465833722, -0.04109649397441243, -0.34051579587487146, -0.4862935063921744, -0.4862935063921744, 0.23880735482613769, -0.4862935063921744, -0.34051579587487146, -0.22733427339809742, -0.3571551671596872, 0.10468121654289057, 0.47964931048271076, 0.10468121654289057, 0.3093683478147389, 0.47964931048271076, -0.04109649397441243, -0.22733427339809742, -0.07725131835913715, -0.4441770406270886, 0.09302964430883469, -0.5433929465833722, -0.34051579587487146, -0.22733427339809742, -0.07725131835913715, -0.08155656288079444, -0.29839933010978564, -0.4441770406270886, -0.29839933010978564, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, 0.030774421492272958, -0.030774421492272958, -0.030774421492272958, -0.030774421492272958, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5174304368920004, 0.6513895306913854, -0.4182145309357168, -0.5174304368920004, 0.11899215400020649, -0.4182145309357168, -0.19706651918506823, 0.13064372623426238, -0.2013717637067256, -0.2013717637067256, -0.4603309967008026, 0.5056118201740826, 0.11899215400020649, -0.2013717637067256, 0.5056118201740826, 0.2647698645175095, 0.11899215400020649, -0.19706651918506823, -0.4603309967008026, -0.2013717637067256, 0.21284484513476587, 0.6513895306913854, -0.05128880866776535, -0.5174304368920004, -0.4603309967008026, 0.11899215400020649, 0.11899215400020649, -0.31455328618349965, 0.13064372623426238, -0.2724368204184138, -0.4182145309357168, -0.3311926574683154, -0.3311926574683154, -0.015133984283040629, 0.11899215400020649, -0.2724368204184138, 0.13064372623426238, -0.3311926574683154, 0.5056118201740826, -0.4603309967008026, 0.11899215400020649, 0.11899215400020649, 0.11899215400020649, 0.5056118201740826, -0.015133984283040629, -0.3311926574683154, 0.5056118201740826, -0.05128880866776535, 0.11899215400020649, 0.5056118201740826, -0.19706651918506823, -0.5174304368920004, -0.31455328618349965, 0.13064372623426238, -0.1854149469510124, -0.015133984283040629, 0.11899215400020649, 0.11899215400020649, -0.05128880866776535, -0.4603309967008026, 0.11899215400020649, -0.4603309967008026, 0.2647698645175095, -0.31455328618349965, -0.015133984283040629, 0.13064372623426238, -0.2013717637067256, -0.4603309967008026, -0.31455328618349965, -0.4182145309357168, -0.015133984283040629, 0.5056118201740826, -0.5174304368920004, -0.015133984283040629, -0.31455328618349965, -0.4603309967008026, -0.4603309967008026, 0.2647698645175095, -0.4603309967008026, -0.31455328618349965, -0.2013717637067256, -0.3311926574683154, 0.13064372623426238, 0.5056118201740826, 0.13064372623426238, 0.3353308575061107, 0.5056118201740826, -0.015133984283040629, -0.25329678308946924, -0.05128880866776535, -0.4182145309357168, 0.11899215400020649, -0.5174304368920004, -0.31455328618349965, -0.2013717637067256, -0.05128880866776535, -0.055594053189422635, -0.2724368204184138, -0.4182145309357168, -0.2724368204184138, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, 0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, 0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, 0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, 0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, -0.024437285972730066, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.48665601539972747, 0.6206151091991124, -0.44898895242798975, -0.48665601539972747, 0.08821773250793354, -0.44898895242798975, -0.2278409406773412, 0.09986930474198942, -0.17059734221445264, -0.17059734221445264, -0.4911054181930755, 0.47483739868180963, 0.08821773250793354, -0.17059734221445264, 0.47483739868180963, 0.23399544302523653, 0.08821773250793354, -0.2278409406773412, -0.4911054181930755, -0.17059734221445264, 0.1820704236424929, 0.6206151091991124, -0.0820632301600383, -0.48665601539972747, -0.4911054181930755, 0.08821773250793354, 0.08821773250793354, -0.3453277076757726, 0.09986930474198942, -0.30321124191068677, -0.44898895242798975, -0.36196707896058833, -0.36196707896058833, -0.04590840577531359, 0.08821773250793354, -0.30321124191068677, 0.09986930474198942, -0.36196707896058833, 0.47483739868180963, -0.4911054181930755, 0.08821773250793354, 0.08821773250793354, 0.08821773250793354, 0.47483739868180963, -0.04590840577531359, -0.36196707896058833, 0.47483739868180963, -0.0820632301600383, 0.08821773250793354, 0.47483739868180963, -0.2278409406773412, -0.48665601539972747, -0.3453277076757726, 0.09986930474198942, -0.21618936844328537, -0.04590840577531359, 0.08821773250793354, 0.08821773250793354, -0.0820632301600383, -0.4911054181930755, 0.08821773250793354, -0.4911054181930755, 0.23399544302523653, -0.3453277076757726, -0.04590840577531359, 0.09986930474198942, -0.17059734221445264, -0.4911054181930755, -0.3453277076757726, -0.44898895242798975, -0.04590840577531359, 0.47483739868180963, -0.48665601539972747, -0.04590840577531359, -0.3453277076757726, -0.4911054181930755, -0.4911054181930755, 0.23399544302523653, -0.4911054181930755, -0.3453277076757726, -0.17059734221445264, -0.36196707896058833, 0.09986930474198942, 0.47483739868180963, 0.09986930474198942, 0.30455643601383775, 0.47483739868180963, -0.04590840577531359, -0.22252236159719627, -0.0820632301600383, -0.44898895242798975, 0.08821773250793354, -0.48665601539972747, -0.3453277076757726, -0.17059734221445264, -0.0820632301600383, -0.024819631697149677, -0.30321124191068677, -0.44898895242798975, -0.30321124191068677, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, -0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, -0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, 0.027547467959439704, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5110933013724576, 0.5961778232263824, -0.47342623840071985, -0.5110933013724576, 0.1126550184806636, -0.47342623840071985, -0.20340365470461114, 0.07543201876925935, -0.1950346281871827, -0.1950346281871827, -0.5155427041658056, 0.45040011270907954, 0.1126550184806636, -0.1950346281871827, 0.45040011270907954, 0.2584327289979666, 0.1126550184806636, -0.20340365470461114, -0.5155427041658056, -0.1950346281871827, 0.20650770961522297, 0.5961778232263824, -0.05762594418730824, -0.5110933013724576, -0.5155427041658056, 0.1126550184806636, 0.1126550184806636, -0.3697649936485027, 0.07543201876925935, -0.3276485278834168, -0.47342623840071985, -0.3864043649333184, -0.3864043649333184, -0.07034569174804366, 0.1126550184806636, -0.3276485278834168, 0.07543201876925935, -0.3864043649333184, 0.45040011270907954, -0.5155427041658056, 0.1126550184806636, 0.1126550184806636, 0.1126550184806636, 0.45040011270907954, -0.07034569174804366, -0.3864043649333184, 0.45040011270907954, -0.05762594418730824, 0.1126550184806636, 0.45040011270907954, -0.20340365470461114, -0.5110933013724576, -0.3697649936485027, 0.07543201876925935, -0.24062665441601544, -0.07034569174804366, 0.1126550184806636, 0.1126550184806636, -0.05762594418730824, -0.5155427041658056, 0.1126550184806636, -0.5155427041658056, 0.2584327289979666, -0.3697649936485027, -0.07034569174804366, 0.07543201876925935, -0.1950346281871827, -0.5155427041658056, -0.3697649936485027, -0.47342623840071985, -0.07034569174804366, 0.45040011270907954, -0.5110933013724576, -0.07034569174804366, -0.3697649936485027, -0.5155427041658056, -0.5155427041658056, 0.2584327289979666, -0.5155427041658056, -0.3697649936485027, -0.1950346281871827, -0.3864043649333184, 0.07543201876925935, 0.45040011270907954, 0.07543201876925935, 0.28011915004110766, 0.45040011270907954, -0.07034569174804366, -0.24695964756992633, -0.05762594418730824, -0.47342623840071985, 0.1126550184806636, -0.5110933013724576, -0.3697649936485027, -0.1950346281871827, -0.05762594418730824, -0.049256917669879746, -0.3276485278834168, -0.47342623840071985, -0.3276485278834168, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, 0.02217501979828797, -0.02217501979828797, -0.02217501979828797, -0.02217501979828797, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4835458334130179, 0.6237252911858221, -0.44587877044128016, -0.4835458334130179, 0.1402024864401033, -0.44587877044128016, -0.17585618674517142, 0.10297948672869905, -0.167487160227743, -0.167487160227743, -0.4879952362063659, 0.4779475806685192, 0.1402024864401033, -0.167487160227743, 0.4779475806685192, 0.2859801969574063, 0.1402024864401033, -0.17585618674517142, -0.4879952362063659, -0.167487160227743, 0.17896024165578325, 0.6237252911858221, -0.030078476227868533, -0.4835458334130179, -0.4879952362063659, 0.1402024864401033, 0.1402024864401033, -0.342217525689063, 0.10297948672869905, -0.3001010599239771, -0.44587877044128016, -0.35885689697387874, -0.35885689697387874, -0.042798223788603956, 0.1402024864401033, -0.3001010599239771, 0.10297948672869905, -0.35885689697387874, 0.4779475806685192, -0.4879952362063659, 0.1402024864401033, 0.1402024864401033, 0.1402024864401033, 0.4779475806685192, -0.042798223788603956, -0.35885689697387874, 0.4779475806685192, -0.030078476227868533, 0.1402024864401033, 0.4779475806685192, -0.17585618674517142, -0.4835458334130179, -0.342217525689063, 0.10297948672869905, -0.21307918645657573, -0.042798223788603956, 0.1402024864401033, 0.1402024864401033, -0.030078476227868533, -0.4879952362063659, 0.1402024864401033, -0.4879952362063659, 0.2859801969574063, -0.342217525689063, -0.042798223788603956, 0.10297948672869905, -0.167487160227743, -0.4879952362063659, -0.342217525689063, -0.44587877044128016, -0.042798223788603956, 0.4779475806685192, -0.4835458334130179, -0.042798223788603956, -0.342217525689063, -0.4879952362063659, -0.4879952362063659, 0.2859801969574063, -0.4879952362063659, -0.342217525689063, -0.167487160227743, -0.35885689697387874, 0.10297948672869905, 0.4779475806685192, 0.10297948672869905, 0.30766661800054734, 0.4779475806685192, -0.042798223788603956, -0.274507115529366, -0.030078476227868533, -0.44587877044128016, 0.1402024864401033, -0.4835458334130179, -0.342217525689063, -0.167487160227743, -0.030078476227868533, -0.021709449710440042, -0.3001010599239771, -0.44587877044128016, -0.3001010599239771, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, -0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, -0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, 0.017345529206842427, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5057208532113059, 0.6015502713875341, -0.4237037506429922, -0.5057208532113059, 0.11802746664181533, -0.4680537902395681, -0.1980312065434594, 0.08080446693041109, -0.18966218002603097, -0.18966218002603097, -0.5101702560046539, 0.45577256087023127, 0.11802746664181533, -0.18966218002603097, 0.45577256087023127, 0.26380517715911833, 0.11802746664181533, -0.1980312065434594, -0.5101702560046539, -0.18966218002603097, 0.15678522185749527, 0.6015502713875341, -0.052253496026156504, -0.5057208532113059, -0.5101702560046539, 0.11802746664181533, 0.11802746664181533, -0.36439254548735095, 0.08080446693041109, -0.3222760797222651, -0.4680537902395681, -0.3810319167721667, -0.3810319167721667, -0.06497324358689192, 0.16237750623839128, -0.3222760797222651, 0.08080446693041109, -0.3810319167721667, 0.45577256087023127, -0.5101702560046539, 0.11802746664181533, 0.11802746664181533, 0.11802746664181533, 0.45577256087023127, -0.06497324358689192, -0.3366818771755908, 0.45577256087023127, -0.052253496026156504, 0.11802746664181533, 0.45577256087023127, -0.1980312065434594, -0.5057208532113059, -0.32004250589077504, 0.08080446693041109, -0.2352542062548637, -0.06497324358689192, 0.11802746664181533, 0.11802746664181533, -0.052253496026156504, -0.5101702560046539, 0.11802746664181533, -0.5101702560046539, 0.26380517715911833, -0.36439254548735095, -0.020623203990315985, 0.08080446693041109, -0.145312140429455, -0.5101702560046539, -0.36439254548735095, -0.4680537902395681, -0.020623203990315985, 0.45577256087023127, -0.5057208532113059, -0.06497324358689192, -0.36439254548735095, -0.5101702560046539, -0.5101702560046539, 0.26380517715911833, -0.5101702560046539, -0.36439254548735095, -0.18966218002603097, -0.3810319167721667, 0.08080446693041109, 0.45577256087023127, 0.08080446693041109, 0.2854915982022594, 0.5001226004668072, -0.06497324358689192, -0.296682135327654, -0.052253496026156504, -0.4680537902395681, 0.11802746664181533, -0.5057208532113059, -0.36439254548735095, -0.18966218002603097, -0.052253496026156504, 0.0004655700878479288, -0.3222760797222651, -0.4680537902395681, -0.3222760797222651, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, 0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, 0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, -0.019334134276650303, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4883753240044635, 0.6188958005943765, -0.4063582214361498, -0.4883753240044635, 0.13537299584865775, -0.4507082610327257, -0.18068567733661697, 0.09814999613725352, -0.17231665081918854, -0.17231665081918854, -0.49282472679781153, 0.47311809007707367, 0.13537299584865775, -0.17231665081918854, 0.47311809007707367, 0.28115070636596073, 0.13537299584865775, -0.18068567733661697, -0.49282472679781153, -0.17231665081918854, 0.13943969265065284, 0.6188958005943765, -0.034907966819314074, -0.4883753240044635, -0.49282472679781153, 0.13537299584865775, 0.13537299584865775, -0.34704701628050855, 0.09814999613725352, -0.30493055051542267, -0.4507082610327257, -0.3636863875653243, -0.3636863875653243, -0.04762771438004949, 0.17972303544523371, -0.30493055051542267, 0.09814999613725352, -0.3636863875653243, 0.47311809007707367, -0.49282472679781153, 0.13537299584865775, 0.13537299584865775, 0.13537299584865775, 0.47311809007707367, -0.04762771438004949, -0.3193363479687484, 0.47311809007707367, -0.034907966819314074, 0.13537299584865775, 0.47311809007707367, -0.18068567733661697, -0.4883753240044635, -0.30269697668393264, 0.09814999613725352, -0.21790867704802128, -0.04762771438004949, 0.13537299584865775, 0.13537299584865775, -0.034907966819314074, -0.49282472679781153, 0.13537299584865775, -0.49282472679781153, 0.28115070636596073, -0.34704701628050855, -0.003277674783473558, 0.09814999613725352, -0.12796661122261258, -0.49282472679781153, -0.34704701628050855, -0.4507082610327257, -0.003277674783473558, 0.47311809007707367, -0.4883753240044635, -0.04762771438004949, -0.34704701628050855, -0.49282472679781153, -0.49282472679781153, 0.28115070636596073, -0.49282472679781153, -0.34704701628050855, -0.17231665081918854, -0.3636863875653243, 0.09814999613725352, 0.47311809007707367, 0.09814999613725352, 0.3028371274091018, 0.5174681296736496, -0.04762771438004949, -0.3140276645344964, -0.034907966819314074, -0.4507082610327257, 0.13537299584865775, -0.4883753240044635, -0.34704701628050855, -0.17231665081918854, -0.034907966819314074, 0.017811099294690356, -0.30493055051542267, -0.4507082610327257, -0.30493055051542267, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, 0.014652160100703162, -0.014652160100703162, 0.014652160100703162, 0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, -0.014652160100703162, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5077094582811138, 0.5995616663177262, -0.4256923557128001, -0.5077094582811138, 0.11603886157200745, -0.470042395309376, -0.20001981161326726, 0.11748413041390382, -0.19165078509583883, -0.19165078509583883, -0.5121588610744618, 0.45378395580042336, 0.11603886157200745, -0.19165078509583883, 0.45378395580042336, 0.2618165720893104, 0.11603886157200745, -0.20001981161326726, -0.5121588610744618, -0.19165078509583883, 0.12010555837400254, 0.5995616663177262, -0.054242101095964376, -0.5077094582811138, -0.5121588610744618, 0.11603886157200745, 0.11603886157200745, -0.36638115055715886, 0.11748413041390382, -0.324264684792073, -0.470042395309376, -0.34435225328867397, -0.34435225328867397, -0.02829358010339919, 0.1603889011685834, -0.324264684792073, 0.11748413041390382, -0.34435225328867397, 0.45378395580042336, -0.5121588610744618, 0.11603886157200745, 0.11603886157200745, 0.11603886157200745, 0.45378395580042336, -0.02829358010339919, -0.30000221369209806, 0.45378395580042336, -0.054242101095964376, 0.11603886157200745, 0.45378395580042336, -0.20001981161326726, -0.5077094582811138, -0.32203111096058296, 0.11748413041390382, -0.198574542771371, -0.02829358010339919, 0.11603886157200745, 0.11603886157200745, -0.054242101095964376, -0.5121588610744618, 0.11603886157200745, -0.5121588610744618, 0.2618165720893104, -0.36638115055715886, 0.016056459493176745, 0.11748413041390382, -0.14730074549926286, -0.5121588610744618, -0.36638115055715886, -0.470042395309376, 0.016056459493176745, 0.45378395580042336, -0.5077094582811138, -0.02829358010339919, -0.36638115055715886, -0.5121588610744618, -0.5121588610744618, 0.2618165720893104, -0.5121588610744618, -0.36638115055715886, -0.19165078509583883, -0.34435225328867397, 0.11748413041390382, 0.45378395580042336, 0.11748413041390382, 0.2835029931324515, 0.4981339953969993, -0.02829358010339919, -0.3333617988111467, -0.054242101095964376, -0.470042395309376, 0.11603886157200745, -0.5077094582811138, -0.36638115055715886, -0.19165078509583883, -0.054242101095964376, -0.0015230349819599469, -0.324264684792073, -0.470042395309376, -0.324264684792073, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.01962424937665444, 0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, 0.01962424937665444, -0.01962424937665444, -0.01962424937665444, -0.01962424937665444, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5223616183818169, 0.6142138264184294, -0.44034451581350326, -0.5223616183818169, 0.10138670147130428, -0.48469455541007916, -0.21467197171397043, 0.10283197031320065, -0.206302945196542, -0.206302945196542, -0.526811021175165, 0.4684361159011265, 0.10138670147130428, -0.206302945196542, 0.4684361159011265, 0.24716441198860725, 0.10138670147130428, -0.21467197171397043, -0.526811021175165, -0.206302945196542, 0.10545339827329937, 0.6142138264184294, -0.06889426119666754, -0.5223616183818169, -0.526811021175165, 0.10138670147130428, 0.10138670147130428, -0.381033310657862, 0.10283197031320065, -0.3389168448927761, -0.48469455541007916, -0.3590044133893771, -0.3590044133893771, -0.042945740204102356, 0.14573674106788023, -0.3389168448927761, 0.10283197031320065, -0.3590044133893771, 0.4684361159011265, -0.526811021175165, 0.10138670147130428, 0.10138670147130428, 0.10138670147130428, 0.4684361159011265, -0.042945740204102356, -0.3146543737928012, 0.4684361159011265, -0.06889426119666754, 0.10138670147130428, 0.4684361159011265, -0.21467197171397043, -0.5223616183818169, -0.3366832710612861, 0.10283197031320065, -0.21322670287207415, -0.042945740204102356, 0.10138670147130428, 0.10138670147130428, -0.06889426119666754, -0.526811021175165, 0.10138670147130428, -0.526811021175165, 0.24716441198860725, -0.381033310657862, 0.0014042993924735826, 0.10283197031320065, -0.16195290559996603, -0.526811021175165, -0.381033310657862, -0.48469455541007916, 0.0014042993924735826, 0.4684361159011265, -0.5223616183818169, -0.042945740204102356, -0.381033310657862, -0.526811021175165, -0.526811021175165, 0.24716441198860725, -0.526811021175165, -0.381033310657862, -0.206302945196542, -0.3590044133893771, 0.10283197031320065, 0.4684361159011265, 0.10283197031320065, 0.2981551532331546, 0.5127861554977025, -0.042945740204102356, -0.34801395891184983, -0.06889426119666754, -0.48469455541007916, 0.10138670147130428, -0.5223616183818169, -0.381033310657862, -0.206302945196542, -0.06889426119666754, -0.01617519508266311, -0.3389168448927761, -0.48469455541007916, -0.3389168448927761, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, -0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, -0.020904230648029636, 0.020904230648029636, 0.020904230648029636, 0.020904230648029636, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.5027373690051625, 0.6338380757950838, -0.4599687651901577, -0.5027373690051625, 0.12101095084795872, -0.5043188047867336, -0.195047722337316, 0.1224562196898551, -0.18667869581988755, -0.18667869581988755, -0.5071867717985106, 0.48806036527778096, 0.12101095084795872, -0.18667869581988755, 0.48806036527778096, 0.2667886613652617, 0.12101095084795872, -0.195047722337316, -0.5071867717985106, -0.18667869581988755, 0.12507764764995383, 0.6338380757950838, -0.0492700118200131, -0.5027373690051625, -0.5071867717985106, 0.12101095084795872, 0.12101095084795872, -0.36140906128120753, 0.1224562196898551, -0.3585410942694306, -0.5043188047867336, -0.33938016401272264, -0.33938016401272264, -0.023321490827447916, 0.16536099044453467, -0.3585410942694306, 0.1224562196898551, -0.33938016401272264, 0.48806036527778096, -0.5071867717985106, 0.12101095084795872, 0.12101095084795872, 0.12101095084795872, 0.48806036527778096, -0.023321490827447916, -0.29503012441614673, 0.48806036527778096, -0.0492700118200131, 0.12101095084795872, 0.48806036527778096, -0.195047722337316, -0.5027373690051625, -0.3170590216846316, 0.1224562196898551, -0.1936024534954197, -0.023321490827447916, 0.12101095084795872, 0.12101095084795872, -0.0492700118200131, -0.5071867717985106, 0.12101095084795872, -0.5071867717985106, 0.2667886613652617, -0.36140906128120753, 0.021028548769128023, 0.1224562196898551, -0.1423286562233116, -0.5071867717985106, -0.36140906128120753, -0.5043188047867336, 0.021028548769128023, 0.48806036527778096, -0.5027373690051625, -0.023321490827447916, -0.36140906128120753, -0.5071867717985106, -0.5071867717985106, 0.2667886613652617, -0.5071867717985106, -0.36140906128120753, -0.18667869581988755, -0.33938016401272264, 0.1224562196898551, 0.48806036527778096, 0.1224562196898551, 0.3177794026098091, 0.5324104048743569, -0.023321490827447916, -0.3283897095351954, -0.0492700118200131, -0.5043188047867336, 0.12101095084795872, -0.5027373690051625, -0.36140906128120753, -0.18667869581988755, -0.0492700118200131, 0.0034490542939913313, -0.3585410942694306, -0.5043188047867336, -0.3585410942694306, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, 0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, 0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, 0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, 0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, -0.014384029614426662, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4818331383571329, 0.6129338451470542, -0.43906453454212807, -0.4818331383571329, 0.10010672019992908, -0.483414574138704, -0.17414349168928636, 0.10155198904182546, -0.20758292646791718, -0.20758292646791718, -0.4862825411504809, 0.4671561346297513, 0.10010672019992908, -0.20758292646791718, 0.4671561346297513, 0.24588443071723207, 0.10010672019992908, -0.17414349168928636, -0.4862825411504809, -0.20758292646791718, 0.10417341700192419, 0.6129338451470542, -0.028365781171983465, -0.4818331383571329, -0.4862825411504809, 0.10010672019992908, 0.10010672019992908, -0.3405048306331779, 0.10155198904182546, -0.33763686362140094, -0.483414574138704, -0.318475933364693, -0.318475933364693, -0.044225721475477556, 0.14445675979650505, -0.33763686362140094, 0.10155198904182546, -0.318475933364693, 0.4671561346297513, -0.4862825411504809, 0.10010672019992908, 0.10010672019992908, 0.10010672019992908, 0.4671561346297513, -0.044225721475477556, -0.2741258937681171, 0.4671561346297513, -0.028365781171983465, 0.10010672019992908, 0.4671561346297513, -0.17414349168928636, -0.4818331383571329, -0.29615479103660197, 0.10155198904182546, -0.1726982228473901, -0.044225721475477556, 0.10010672019992908, 0.10010672019992908, -0.028365781171983465, -0.4862825411504809, 0.10010672019992908, -0.4862825411504809, 0.24588443071723207, -0.3405048306331779, 0.00012431812109838652, 0.10155198904182546, -0.16323288687134122, -0.4862825411504809, -0.3405048306331779, -0.483414574138704, 0.00012431812109838652, 0.4671561346297513, -0.4818331383571329, -0.044225721475477556, -0.3405048306331779, -0.4862825411504809, -0.4862825411504809, 0.24588443071723207, -0.4862825411504809, -0.3405048306331779, -0.20758292646791718, -0.318475933364693, 0.10155198904182546, 0.4671561346297513, 0.10155198904182546, 0.33868363325783873, 0.5115061742263273, -0.044225721475477556, -0.34929394018322507, -0.028365781171983465, -0.483414574138704, 0.10010672019992908, -0.4818331383571329, -0.3405048306331779, -0.20758292646791718, -0.028365781171983465, -0.017455176354038305, -0.33763686362140094, -0.483414574138704, -0.33763686362140094, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, -0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, -0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, 0.01748785578819864, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4962171679715595, 0.5985498155326275, -0.4534485641565547, -0.4962171679715595, 0.11449074981435575, -0.4977986037531306, -0.1597594620748597, 0.0871679594273988, -0.22196695608234385, -0.22196695608234385, -0.5006665707649076, 0.45277210501532467, 0.11449074981435575, -0.22196695608234385, 0.45277210501532467, 0.26026846033165874, 0.11449074981435575, -0.1597594620748597, -0.5006665707649076, -0.22196695608234385, 0.11855744661635084, 0.5985498155326275, -0.013981751557556803, -0.4962171679715595, -0.5006665707649076, 0.11449074981435575, 0.11449074981435575, -0.3548888602476045, 0.0871679594273988, -0.3520208932358276, -0.4977986037531306, -0.33285996297911963, -0.33285996297911963, -0.05860975108990422, 0.15884078941093172, -0.3520208932358276, 0.0871679594273988, -0.33285996297911963, 0.45277210501532467, -0.5006665707649076, 0.11449074981435575, 0.11449074981435575, 0.11449074981435575, 0.45277210501532467, -0.05860975108990422, -0.2885099233825437, 0.45277210501532467, -0.013981751557556803, 0.11449074981435575, 0.45277210501532467, -0.1597594620748597, -0.4962171679715595, -0.3105388206510286, 0.0871679594273988, -0.18708225246181676, -0.05860975108990422, 0.11449074981435575, 0.11449074981435575, -0.013981751557556803, -0.5006665707649076, 0.11449074981435575, -0.5006665707649076, 0.26026846033165874, -0.3548888602476045, -0.014259711493328275, 0.0871679594273988, -0.17761691648576788, -0.5006665707649076, -0.3548888602476045, -0.4977986037531306, -0.014259711493328275, 0.45277210501532467, -0.4962171679715595, -0.05860975108990422, -0.3548888602476045, -0.5006665707649076, -0.5006665707649076, 0.26026846033165874, -0.5006665707649076, -0.3548888602476045, -0.22196695608234385, -0.33285996297911963, 0.0871679594273988, 0.45277210501532467, 0.0871679594273988, 0.3242996036434121, 0.49712214461190063, -0.05860975108990422, -0.3636779697976517, -0.013981751557556803, -0.4977986037531306, 0.11449074981435575, -0.4962171679715595, -0.3548888602476045, -0.22196695608234385, -0.013981751557556803, -0.03183920596846497, -0.3520208932358276, -0.4977986037531306, -0.3520208932358276, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, 0.017321692640262277, -0.017321692640262277, -0.017321692640262277, -0.017321692640262277, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4787293121833609, 0.6160376713208262, -0.4359607083683561, -0.4787293121833609, 0.1319786056025544, -0.480310747964932, -0.14227160628666105, 0.10465581521559744, -0.2044791002941452, -0.2044791002941452, -0.483178714976709, 0.4702599608035233, 0.1319786056025544, -0.2044791002941452, 0.4702599608035233, 0.27775631611985735, 0.1319786056025544, -0.14227160628666105, -0.483178714976709, -0.2044791002941452, 0.1010695908281522, 0.6160376713208262, 0.003506104230641835, -0.4787293121833609, -0.483178714976709, 0.1319786056025544, 0.1319786056025544, -0.3374010044594059, 0.10465581521559744, -0.33453303744762897, -0.480310747964932, -0.315372107190921, -0.315372107190921, -0.04112189530170558, 0.17632864519913036, -0.33453303744762897, 0.10465581521559744, -0.315372107190921, 0.4702599608035233, -0.483178714976709, 0.1319786056025544, 0.1319786056025544, 0.1319786056025544, 0.4702599608035233, -0.04112189530170558, -0.2710220675943451, 0.4702599608035233, 0.003506104230641835, 0.1319786056025544, 0.4702599608035233, -0.14227160628666105, -0.4787293121833609, -0.29305096486283, 0.10465581521559744, -0.16959439667361811, -0.04112189530170558, 0.1319786056025544, 0.1319786056025544, 0.003506104230641835, -0.483178714976709, 0.1319786056025544, -0.483178714976709, 0.27775631611985735, -0.3374010044594059, 0.003228144294870363, 0.10465581521559744, -0.16012906069756924, -0.483178714976709, -0.3374010044594059, -0.480310747964932, 0.003228144294870363, 0.4702599608035233, -0.4787293121833609, -0.04112189530170558, -0.3374010044594059, -0.483178714976709, -0.483178714976709, 0.27775631611985735, -0.483178714976709, -0.3374010044594059, -0.2044791002941452, -0.315372107190921, 0.10465581521559744, 0.4702599608035233, 0.10465581521559744, 0.3417874594316107, 0.5146100004000993, -0.04112189530170558, -0.3811658255858503, 0.003506104230641835, -0.480310747964932, 0.1319786056025544, -0.4787293121833609, -0.3374010044594059, -0.2044791002941452, 0.003506104230641835, -0.014351350180266332, -0.33453303744762897, -0.480310747964932, -0.33453303744762897, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.011341390494266072, 0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, 0.011341390494266072, -0.011341390494266072, -0.011341390494266072, -0.011341390494266072, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4614076195430986, 0.5987159786805639, -0.4532824010086184, -0.4614076195430986, 0.11465691296229212, -0.4976324406051943, -0.15959329892692334, 0.08733412257533517, -0.18715740765388292, -0.18715740765388292, -0.5005004076169712, 0.452938268163261, 0.11465691296229212, -0.18715740765388292, 0.452938268163261, 0.26043462347959506, 0.11465691296229212, -0.15959329892692334, -0.5005004076169712, -0.18715740765388292, 0.08374789818788993, 0.5987159786805639, -0.013815588409620442, -0.4614076195430986, -0.5005004076169712, 0.11465691296229212, 0.11465691296229212, -0.3547226970996682, 0.08733412257533517, -0.35185473008789125, -0.4976324406051943, -0.3326937998311833, -0.3326937998311833, -0.058443587941967856, 0.15900695255886807, -0.35185473008789125, 0.08733412257533517, -0.3326937998311833, 0.452938268163261, -0.5005004076169712, 0.11465691296229212, 0.11465691296229212, 0.11465691296229212, 0.452938268163261, -0.058443587941967856, -0.2883437602346074, 0.452938268163261, -0.013815588409620442, 0.11465691296229212, 0.452938268163261, -0.15959329892692334, -0.4614076195430986, -0.3103726575030923, 0.08733412257533517, -0.1869160893138804, -0.058443587941967856, 0.11465691296229212, 0.11465691296229212, -0.013815588409620442, -0.5005004076169712, 0.11465691296229212, -0.5005004076169712, 0.26043462347959506, -0.3547226970996682, -0.014093548345391914, 0.08733412257533517, -0.14280736805730695, -0.5005004076169712, -0.3547226970996682, -0.4976324406051943, -0.014093548345391914, 0.452938268163261, -0.4614076195430986, -0.058443587941967856, -0.3547226970996682, -0.5005004076169712, -0.5005004076169712, 0.26043462347959506, -0.5005004076169712, -0.3547226970996682, -0.18715740765388292, -0.3326937998311833, 0.08733412257533517, 0.452938268163261, 0.08733412257533517, 0.3244657667913484, 0.497288307759837, -0.058443587941967856, -0.36384413294558804, -0.013815588409620442, -0.4976324406051943, 0.11465691296229212, -0.4614076195430986, -0.3547226970996682, -0.18715740765388292, -0.013815588409620442, 0.002970342459995945, -0.35185473008789125, -0.4976324406051943, -0.35185473008789125, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, 0.017916047935579344, -0.017916047935579344, 0.017916047935579344, 0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, -0.017916047935579344, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.45006622904883253, 0.61005736917483, -0.46462379150288446, -0.45006622904883253, 0.12599830345655819, -0.5089738310994604, -0.14825190843265726, 0.09867551306960123, -0.17581601715961684, -0.17581601715961684, -0.48915901712270515, 0.4642796586575271, 0.12599830345655819, -0.17581601715961684, 0.4642796586575271, 0.27177601397386114, 0.12599830345655819, -0.14825190843265726, -0.48915901712270515, -0.17581601715961684, 0.09508928868215599, 0.61005736917483, -0.0024741979153543696, -0.45006622904883253, -0.48915901712270515, 0.12599830345655819, 0.12599830345655819, -0.3433813066054021, 0.09867551306960123, -0.36319612058215733, -0.5089738310994604, -0.3213524093369172, -0.3213524093369172, -0.047102197447701784, 0.17034834305313415, -0.36319612058215733, 0.09867551306960123, -0.3213524093369172, 0.4642796586575271, -0.48915901712270515, 0.12599830345655819, 0.12599830345655819, 0.12599830345655819, 0.4642796586575271, -0.047102197447701784, -0.2770023697403413, 0.4642796586575271, -0.0024741979153543696, 0.12599830345655819, 0.4642796586575271, -0.14825190843265726, -0.45006622904883253, -0.2990312670088262, 0.09867551306960123, -0.17557469881961432, -0.047102197447701784, 0.12599830345655819, 0.12599830345655819, -0.0024741979153543696, -0.48915901712270515, 0.12599830345655819, -0.48915901712270515, 0.27177601397386114, -0.3433813066054021, -0.0027521578511258415, 0.09867551306960123, -0.13146597756304088, -0.48915901712270515, -0.3433813066054021, -0.5089738310994604, -0.0027521578511258415, 0.4642796586575271, -0.45006622904883253, -0.047102197447701784, -0.3433813066054021, -0.48915901712270515, -0.48915901712270515, 0.27177601397386114, -0.48915901712270515, -0.3433813066054021, -0.17581601715961684, -0.3213524093369172, 0.09867551306960123, 0.4642796586575271, 0.09867551306960123, 0.3358071572856145, 0.5086296982541031, -0.047102197447701784, -0.35250274245132196, -0.0024741979153543696, -0.5089738310994604, 0.12599830345655819, -0.45006622904883253, -0.3433813066054021, -0.17581601715961684, -0.0024741979153543696, 0.014311732954262017, -0.36319612058215733, -0.5089738310994604, -0.36319612058215733, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, -0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, -0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, 0.013086015871120614, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.46798227698441186, 0.6279734171104093, -0.4825398394384638, -0.46798227698441186, 0.10808225552097885, -0.5268898790350397, -0.1661679563682366, 0.0807594651340219, -0.1937320650951962, -0.1937320650951962, -0.5070750650582845, 0.4821957065931064, 0.10808225552097885, -0.1937320650951962, 0.4821957065931064, 0.2538599660382818, 0.10808225552097885, -0.1661679563682366, -0.5070750650582845, -0.1937320650951962, 0.07717324074657665, 0.6279734171104093, -0.020390245850933714, -0.46798227698441186, -0.5070750650582845, 0.10808225552097885, 0.10808225552097885, -0.36129735454098144, 0.0807594651340219, -0.38111216851773666, -0.5268898790350397, -0.33926845727249655, -0.33926845727249655, -0.06501824538328113, 0.1524322951175548, -0.38111216851773666, 0.0807594651340219, -0.33926845727249655, 0.4821957065931064, -0.5070750650582845, 0.10808225552097885, 0.10808225552097885, 0.10808225552097885, 0.4821957065931064, -0.06501824538328113, -0.29491841767592064, 0.4821957065931064, -0.020390245850933714, 0.10808225552097885, 0.4821957065931064, -0.1661679563682366, -0.46798227698441186, -0.31694731494440553, 0.0807594651340219, -0.19349074675519368, -0.06501824538328113, 0.10808225552097885, 0.10808225552097885, -0.020390245850933714, -0.5070750650582845, 0.10808225552097885, -0.5070750650582845, 0.2538599660382818, -0.36129735454098144, -0.020668205786705186, 0.0807594651340219, -0.14938202549862023, -0.5070750650582845, -0.36129735454098144, -0.5268898790350397, -0.020668205786705186, 0.4821957065931064, -0.46798227698441186, -0.06501824538328113, -0.36129735454098144, -0.5070750650582845, -0.5070750650582845, 0.2538599660382818, -0.5070750650582845, -0.36129735454098144, -0.1937320650951962, -0.33926845727249655, 0.0807594651340219, 0.4821957065931064, 0.0807594651340219, 0.3537232052211938, 0.5265457461896824, -0.06501824538328113, -0.3704187903869013, -0.020390245850933714, -0.5268898790350397, 0.10808225552097885, -0.46798227698441186, -0.36129735454098144, -0.1937320650951962, -0.020390245850933714, -0.003604314981317327, -0.38111216851773666, -0.5268898790350397, -0.38111216851773666, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, 0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, -0.014564409290199434, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.45489626111329123, 0.6410594329815299, -0.46945382356734316, -0.45489626111329123, 0.12116827139209946, -0.5138038631639191, -0.153081940497116, 0.0938454810051425, -0.18064604922407557, -0.18064604922407557, -0.49398904918716385, 0.495281722464227, 0.12116827139209946, -0.18064604922407557, 0.495281722464227, 0.26694598190940244, 0.12116827139209946, -0.153081940497116, -0.49398904918716385, -0.18064604922407557, 0.06408722487545604, 0.6410594329815299, -0.007304229979813099, -0.45489626111329123, -0.49398904918716385, 0.12116827139209946, 0.12116827139209946, -0.3482113386698608, 0.0938454810051425, -0.36802615264661603, -0.5138038631639191, -0.3261824414013759, -0.3261824414013759, -0.05193222951216052, 0.16551831098867542, -0.36802615264661603, 0.0938454810051425, -0.3261824414013759, 0.495281722464227, -0.49398904918716385, 0.12116827139209946, 0.12116827139209946, 0.12116827139209946, 0.495281722464227, -0.05193222951216052, -0.2818324018048, 0.495281722464227, -0.007304229979813099, 0.12116827139209946, 0.495281722464227, -0.153081940497116, -0.45489626111329123, -0.3038612990732849, 0.0938454810051425, -0.18040473088407305, -0.05193222951216052, 0.12116827139209946, 0.12116827139209946, -0.007304229979813099, -0.49398904918716385, 0.12116827139209946, -0.49398904918716385, 0.26694598190940244, -0.3482113386698608, -0.007582189915584571, 0.0938454810051425, -0.1362960096274996, -0.49398904918716385, -0.3482113386698608, -0.5138038631639191, -0.007582189915584571, 0.495281722464227, -0.45489626111329123, -0.05193222951216052, -0.3482113386698608, -0.49398904918716385, -0.49398904918716385, 0.26694598190940244, -0.49398904918716385, -0.3482113386698608, -0.18064604922407557, -0.3261824414013759, 0.0938454810051425, 0.495281722464227, 0.0938454810051425, 0.36680922109231445, 0.539631762060803, -0.05193222951216052, -0.3835048062580219, -0.007304229979813099, -0.5138038631639191, 0.12116827139209946, -0.45489626111329123, -0.3482113386698608, -0.18064604922407557, -0.007304229979813099, 0.009481700889803287, -0.36802615264661603, -0.5138038631639191, -0.36802615264661603, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.011702579944675011, 0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, 0.011702579944675011, -0.011702579944675011, -0.011702579944675011, -0.011702579944675011, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4403318518230918, 0.6264950236913305, -0.4840182328575426, -0.46946067040349065, 0.13573268068229888, -0.5283682724541185, -0.1676463497873154, 0.07928107171494307, -0.195210458514275, -0.195210458514275, -0.5085534584773633, 0.4807173131740276, 0.10660386210190002, -0.195210458514275, 0.4807173131740276, 0.252381572619203, 0.13573268068229888, -0.1676463497873154, -0.5085534584773633, -0.195210458514275, 0.049522815585256605, 0.6264950236913305, -0.021868639270012535, -0.46946067040349065, -0.5085534584773633, 0.10660386210190002, 0.10660386210190002, -0.36277574796006024, 0.07928107171494307, -0.38259056193681545, -0.5283682724541185, -0.34074685069157534, -0.34074685069157534, -0.06649663880235995, 0.150953901698476, -0.38259056193681545, 0.07928107171494307, -0.34074685069157534, 0.4807173131740276, -0.5085534584773633, 0.10660386210190002, 0.13573268068229888, 0.10660386210190002, 0.5098461317544265, -0.06649663880235995, -0.29639681109499944, 0.5098461317544265, -0.021868639270012535, 0.13573268068229888, 0.4807173131740276, -0.1676463497873154, -0.46946067040349065, -0.3184257083634843, 0.07928107171494307, -0.19496914017427247, -0.06649663880235995, 0.10660386210190002, 0.13573268068229888, -0.021868639270012535, -0.5085534584773633, 0.10660386210190002, -0.5085534584773633, 0.252381572619203, -0.36277574796006024, -0.022146599205784004, 0.07928107171494307, -0.15086041891769902, -0.5085534584773633, -0.36277574796006024, -0.4992394538737197, -0.022146599205784004, 0.4807173131740276, -0.46946067040349065, -0.06649663880235995, -0.36277574796006024, -0.5085534584773633, -0.5085534584773633, 0.252381572619203, -0.5085534584773633, -0.36277574796006024, -0.16608163993387615, -0.34074685069157534, 0.07928107171494307, 0.5098461317544265, 0.07928107171494307, 0.35224481180211503, 0.5250673527706036, -0.06649663880235995, -0.39806921554822133, -0.021868639270012535, -0.5283682724541185, 0.10660386210190002, -0.46946067040349065, -0.36277574796006024, -0.16608163993387615, -0.021868639270012535, -0.005082708400396147, -0.38259056193681545, -0.5283682724541185, -0.38259056193681545, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, 0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, 0.013631478294319502, 0.013631478294319502, -0.013631478294319502, 0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, 0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, 0.013631478294319502, 0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, 0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, -0.013631478294319502, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4286292718784168, 0.6381976036360055, -0.4957208128022176, -0.45775809045881566, 0.1474352606269739, -0.5400708523987936, -0.15594376984264038, 0.09098365165961808, -0.18350787856959996, -0.18350787856959996, -0.49685087853268833, 0.4924198931187026, 0.11830644204657503, -0.18350787856959996, 0.4924198931187026, 0.264084152563878, 0.1474352606269739, -0.15594376984264038, -0.49685087853268833, -0.18350787856959996, 0.061225395529931614, 0.6381976036360055, -0.010166059325337524, -0.45775809045881566, -0.49685087853268833, 0.11830644204657503, 0.11830644204657503, -0.35107316801538524, 0.09098365165961808, -0.39429314188149045, -0.5400708523987936, -0.32904427074690035, -0.32904427074690035, -0.054794058857684944, 0.16265648164315102, -0.39429314188149045, 0.09098365165961808, -0.32904427074690035, 0.4924198931187026, -0.49685087853268833, 0.11830644204657503, 0.1474352606269739, 0.11830644204657503, 0.5215487116991016, -0.054794058857684944, -0.28469423115032444, 0.5215487116991016, -0.010166059325337524, 0.1474352606269739, 0.4924198931187026, -0.15594376984264038, -0.45775809045881566, -0.30672312841880933, 0.09098365165961808, -0.18326656022959745, -0.054794058857684944, 0.11830644204657503, 0.1474352606269739, -0.010166059325337524, -0.49685087853268833, 0.11830644204657503, -0.49685087853268833, 0.264084152563878, -0.35107316801538524, -0.010444019261108993, 0.09098365165961808, -0.139157838973024, -0.49685087853268833, -0.35107316801538524, -0.5109420338183948, -0.010444019261108993, 0.4924198931187026, -0.45775809045881566, -0.054794058857684944, -0.35107316801538524, -0.49685087853268833, -0.49685087853268833, 0.264084152563878, -0.49685087853268833, -0.35107316801538524, -0.15437905998920112, -0.32904427074690035, 0.09098365165961808, 0.5215487116991016, 0.09098365165961808, 0.36394739174679, 0.5367699327152786, -0.054794058857684944, -0.38636663560354634, -0.010166059325337524, -0.5400708523987936, 0.11830644204657503, -0.45775809045881566, -0.35107316801538524, -0.15437905998920112, -0.010166059325337524, 0.006619871544278864, -0.39429314188149045, -0.5400708523987936, -0.39429314188149045, ... ])\n",
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, -0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, -0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, 0.011495143663834332, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.4422607501727363, 0.624566125341686, -0.5093522910965371, -0.47138956875313515, 0.1338037823326544, -0.5537023306931131, -0.1695752481369599, 0.07735217336529858, -0.19713935686391948, -0.19713935686391948, -0.48321940023836885, 0.4787884148243831, 0.10467496375225553, -0.19713935686391948, 0.4787884148243831, 0.25045267426955853, 0.1338037823326544, -0.1695752481369599, -0.5104823568270078, -0.19713935686391948, 0.04759391723561211, 0.624566125341686, -0.023797537619657028, -0.47138956875313515, -0.5104823568270078, 0.10467496375225553, 0.10467496375225553, -0.3647046463097047, 0.07735217336529858, -0.40792462017580994, -0.5537023306931131, -0.34267574904121983, -0.34267574904121983, -0.06842553715200445, 0.1490250033488315, -0.40792462017580994, 0.07735217336529858, -0.34267574904121983, 0.5060513714130221, -0.48321940023836885, 0.10467496375225553, 0.16106673892129342, 0.10467496375225553, 0.507917233404782, -0.06842553715200445, -0.2983257094446439, 0.507917233404782, -0.023797537619657028, 0.1338037823326544, 0.4787884148243831, -0.1695752481369599, -0.47138956875313515, -0.3203546067131288, 0.07735217336529858, -0.19689803852391696, -0.06842553715200445, 0.10467496375225553, 0.1338037823326544, -0.023797537619657028, -0.5104823568270078, 0.10467496375225553, -0.5104823568270078, 0.25045267426955853, -0.3647046463097047, -0.024075497555428496, 0.07735217336529858, -0.15278931726734352, -0.5104823568270078, -0.3647046463097047, -0.49731055552407527, -0.024075497555428496, 0.4787884148243831, -0.47138956875313515, -0.06842553715200445, -0.3647046463097047, -0.48321940023836885, -0.48321940023836885, 0.25045267426955853, -0.5104823568270078, -0.3647046463097047, -0.16801053828352064, -0.34267574904121983, 0.07735217336529858, 0.507917233404782, 0.07735217336529858, 0.35031591345247054, 0.5231384544209591, -0.04116258056336544, -0.3999981138978658, -0.023797537619657028, -0.5537023306931131, 0.10467496375225553, -0.47138956875313515, -0.3647046463097047, -0.16801053828352064, -0.023797537619657028, -0.007011606750040637, -0.40792462017580994, -0.5537023306931131, -0.40792462017580994, ... ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intermediate weighted prediction : ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, 0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, 0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, -0.011303573315333344, ... ])\n",
      "('scores before: ', dtype: float\n",
      "Rows: 9284\n",
      "[-0.430765606508902, 0.6360612690055203, -0.4978571474327028, -0.45989442508930084, 0.14529892599648872, -0.5422071870292788, -0.15808010447312557, 0.08884731702913291, -0.18564421320008515, -0.18564421320008515, -0.47172425657453454, 0.4902835584882174, 0.11617010741608987, -0.18564421320008515, 0.4902835584882174, 0.26194781793339283, 0.14529892599648872, -0.15808010447312557, -0.4989872131631735, -0.18564421320008515, 0.03609877357177778, 0.6360612690055203, -0.012302393955822695, -0.45989442508930084, -0.4989872131631735, 0.11617010741608987, 0.11617010741608987, -0.3532095026458704, 0.08884731702913291, -0.39642947651197563, -0.5422071870292788, -0.33118060537738553, -0.33118060537738553, -0.05693039348817011, 0.16052014701266584, -0.39642947651197563, 0.08884731702913291, -0.33118060537738553, 0.5175465150768565, -0.47172425657453454, 0.11617010741608987, 0.17256188258512775, 0.11617010741608987, 0.5194123770686163, -0.05693039348817011, -0.2868305657808096, 0.5194123770686163, -0.012302393955822695, 0.14529892599648872, 0.4902835584882174, -0.15808010447312557, -0.45989442508930084, -0.3088594630492945, 0.08884731702913291, -0.18540289486008263, -0.05693039348817011, 0.11617010741608987, 0.14529892599648872, -0.012302393955822695, -0.4989872131631735, 0.11617010741608987, -0.4989872131631735, 0.26194781793339283, -0.3532095026458704, -0.012580353891594164, 0.08884731702913291, -0.14129417360350918, -0.4989872131631735, -0.3532095026458704, -0.48581541186024096, -0.012580353891594164, 0.4902835584882174, -0.45989442508930084, -0.05693039348817011, -0.3532095026458704, -0.47172425657453454, -0.47172425657453454, 0.26194781793339283, -0.4989872131631735, -0.3532095026458704, -0.1565153946196863, -0.33118060537738553, 0.08884731702913291, 0.5194123770686163, 0.08884731702913291, 0.36181105711630485, 0.5346335980847934, -0.029667436899531108, -0.41149325756170013, -0.012302393955822695, -0.5422071870292788, 0.11617010741608987, -0.45989442508930084, -0.3532095026458704, -0.1565153946196863, -0.012302393955822695, 0.004483536913793695, -0.39642947651197563, -0.5422071870292788, -0.39642947651197563, ... ])\n",
      "Accuracy of 10-component ensemble = 0.623222748815\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "accuracy = graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.17682363293636694,\n",
       " 0.0931188897118425,\n",
       " 0.07288885525865149,\n",
       " 0.06706306914162356,\n",
       " 0.06456916961624358,\n",
       " 0.05456055779184307,\n",
       " 0.043510936733700704,\n",
       " 0.02898871150035158,\n",
       " 0.025962509691371802]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some of the performance plots dicussed in the lecture.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('scores shape: ', (37224,))\n",
      "Iteration 1, training error = 0.421636578551\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 2, training error = 0.433430045132\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 3, training error = 0.400037610144\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 4, training error = 0.400037610144\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 5, training error = 0.384724908661\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 6, training error = 0.384617451107\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 7, training error = 0.382763808296\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 8, training error = 0.384617451107\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 9, training error = 0.382763808296\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 10, training error = 0.384483129164\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 11, training error = 0.382736943907\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 12, training error = 0.381447453256\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 13, training error = 0.381528046422\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 14, training error = 0.380560928433\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 15, training error = 0.380507199656\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 16, training error = 0.378223726628\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 17, training error = 0.378277455405\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 18, training error = 0.378411777348\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 19, training error = 0.378062540297\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 20, training error = 0.378761014399\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 21, training error = 0.379566946056\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 22, training error = 0.378895336342\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 23, training error = 0.378895336342\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 24, training error = 0.378761014399\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 25, training error = 0.378895336342\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 26, training error = 0.378975929508\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 27, training error = 0.379110251451\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 28, training error = 0.378922200731\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 29, training error = 0.379029658285\n",
      "('scores shape: ', (37224,))\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00507ac150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following best describes a **general trend in accuracy** as we add more and more components? Answer based on the 30 components learned so far.\n",
    "\n",
    "1. Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases.\n",
    "2. Training error goes down in general, with some ups and downs in the middle.\n",
    "3. Training error goes up in general, with some ups and downs in the middle.\n",
    "4. Training error goes down in the beginning, achieves the best error, and then goes up sharply.\n",
    "5. None of the above\n",
    "\n",
    "\n",
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('scores shape: ', (9284,))\n",
      "Iteration 1, test error = 0.42330891857\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 2, test error = 0.428479103835\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 3, test error = 0.398104265403\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 4, test error = 0.398104265403\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 5, test error = 0.379900904782\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 6, test error = 0.380008616975\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 7, test error = 0.379254631624\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 8, test error = 0.380008616975\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 9, test error = 0.379254631624\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 10, test error = 0.379685480396\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 11, test error = 0.379254631624\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 12, test error = 0.377962085308\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 13, test error = 0.379254631624\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 14, test error = 0.377854373115\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 15, test error = 0.378500646273\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 16, test error = 0.377854373115\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 17, test error = 0.377962085308\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 18, test error = 0.377854373115\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 19, test error = 0.378177509694\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 20, test error = 0.376884963378\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 21, test error = 0.377531236536\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 22, test error = 0.376777251185\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 23, test error = 0.376777251185\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 24, test error = 0.376884963378\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 25, test error = 0.376777251185\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 26, test error = 0.376561826799\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 27, test error = 0.376454114606\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 28, test error = 0.376992675571\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 29, test error = 0.376777251185\n",
      "('scores shape: ', (9284,))\n",
      "Iteration 30, test error = 0.376777251185\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f005067a510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
